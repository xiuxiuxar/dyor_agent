# ------------------------------------------------------------------------------
#
#   Copyright 2025 xiuxiuxar
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#
# ------------------------------------------------------------------------------

"""This package contains a behaviour that autogenerated from the protocol ``."""

import os
import re
from abc import ABC
from enum import Enum
from typing import Any
from datetime import UTC, tzinfo, datetime
from concurrent.futures import ThreadPoolExecutor

import markdown  # You may need to add this to your requirements
import requests
import sqlalchemy
from pydantic import ValidationError
from sqlalchemy import text
from aea.skills.behaviours import State, FSMBehaviour

from packages.xiuxiuxar.skills.simple_fsm.prompt import build_report_prompt
from packages.xiuxiuxar.skills.simple_fsm.data_models import (
    NewsItem,
    AssetInfo,
    KeyMetrics,
    TriggerInfo,
    SocialSummary,
    OfficialUpdate,
    OnchainHighlight,
    StructuredPayload,
)
from packages.xiuxiuxar.skills.simple_fsm.data_sources import DATA_SOURCES
from packages.xiuxiuxar.skills.simple_fsm.trendmoon_client import TrendmoonClient, TrendmoonAPIError
from packages.xiuxiuxar.skills.simple_fsm.lookonchain_client import LookOnChainClient, LookOnChainAPIError
from packages.xiuxiuxar.skills.simple_fsm.treeofalpha_client import TreeOfAlphaClient, TreeOfAlphaAPIError
from packages.xiuxiuxar.skills.simple_fsm.researchagent_client import ResearchAgentClient, ResearchAgentAPIError


MAX_WORKERS = 4


class DyorabciappEvents(Enum):
    """Events for the fsm."""

    NO_TRIGGER = "NO_TRIGGER"
    DONE = "DONE"
    RETRY = "RETRY"
    TIMEOUT = "TIMEOUT"
    ERROR = "ERROR"
    TRIGGER = "TRIGGER"


class DyorabciappStates(Enum):
    """States for the fsm."""

    WATCHINGROUND = "watchinground"
    PROCESSDATAROUND = "processdataround"
    SETUPDYORROUND = "setupdyorround"
    DELIVERREPORTROUND = "deliverreportround"
    TRIGGERROUND = "triggerround"
    INGESTDATAROUND = "ingestdataround"
    GENERATEREPORTROUND = "generatereportround"
    HANDLEERRORROUND = "handleerrorround"


class BaseState(State, ABC):
    """Base class for states."""

    _state: DyorabciappStates = None

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._event = None
        self._is_done = False  # Initially, the state is not done

    def act(self) -> None:
        """Perform the act."""
        self._is_done = True
        self._event = DyorabciappEvents.DONE

    def is_done(self) -> bool:
        """Is done."""
        return self._is_done

    @property
    def event(self) -> str | None:
        """Current event."""
        return self._event


# Define states


class WatchingRound(BaseState):
    """This class implements the behaviour of the state WatchingRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.WATCHINGROUND

    def act(self) -> None:
        """Act:
        1. Check if there are any triggers in the database.
        2. If there are, set the event to TRIGGER.
        3. If there are no triggers, set the event to NO_TRIGGER.
        """
        self.context.logger.info(f"Entering state: {self._state}")
        self._event = DyorabciappEvents.TRIGGER

        self._is_done = True  # Mark state as done


class ProcessDataRound(BaseState):
    """This class implements the behaviour of the state ProcessDataRound."""

    def __init__(self, **kwargs) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.PROCESSDATAROUND

    def process_source_data(self, source: str, raw_data: Any, trigger_id: int, asset_id: int) -> dict[str, str]:
        """Process data for a source, handling both single and multi data types."""
        config = DATA_SOURCES.get(source)
        if not config:
            return {source: "No processor configured for this source"}
        is_multi = config["data_type_handler"] == "multi"
        processor_func = getattr(self, config["processor"])
        return self.process_data_type(
            source=source,
            data=raw_data,
            processor_func=processor_func,
            trigger_id=trigger_id,
            asset_id=asset_id,
            is_multi=is_multi,
        )

    def _serialize_for_storage(self, data):
        """Recursively serialize data for storage, leveraging Pydantic where possible."""
        result = data  # Default: return as-is

        if isinstance(data, dict):
            result = {key: self._serialize_for_storage(value) for key, value in data.items()}
        elif isinstance(data, list):
            result = [self._serialize_for_storage(item) for item in data]
        elif isinstance(data, datetime):
            result = data.isoformat()
        elif isinstance(data, tzinfo):
            result = str(data)
        elif hasattr(data, "model_dump"):
            result = data.model_dump(mode="json")
        elif hasattr(data, "__dict__"):
            result = {k: self._serialize_for_storage(v) for k, v in data.__dict__.items() if not k.startswith("_")}

        return result

    def _store_all_raw_data(self, trigger_id, asset_id):
        try:
            for source, config in DATA_SOURCES.items():
                raw = self.context.raw_data.get(source)
                serialized_raw = self._serialize_for_storage(raw)
                self.context.logger.info(
                    "Storing raw data for source=%s, type=%s: "
                    "type(raw)=%s, type(serialized_raw)=%s, raw=%s, serialized_raw=%s",
                    source,
                    "multi" if config.get("data_type_handler") == "multi" else "raw",
                    type(raw),
                    type(serialized_raw),
                    repr(raw)[:500],
                    repr(serialized_raw)[:500],
                )
                if config.get("data_type_handler") == "multi" and isinstance(serialized_raw, dict):
                    for subkey, subval in serialized_raw.items():
                        self.context.logger.info(
                            "Storing multi raw data for source=%s, subkey=%s, type(subval)=%s, subval=%s",
                            source,
                            subkey,
                            type(subval),
                            repr(subval)[:500],
                        )
                        self.context.db_model.store_raw_data(
                            source=source,
                            data_type=subkey,
                            data=subval,
                            trigger_id=trigger_id,
                            timestamp=datetime.now(tz=UTC),
                            asset_id=asset_id,
                        )
                else:
                    self.context.db_model.store_raw_data(
                        source=source,
                        data_type="raw",
                        data=serialized_raw,
                        trigger_id=trigger_id,
                        timestamp=datetime.now(tz=UTC),
                        asset_id=asset_id,
                    )
        except (sqlalchemy.exc.SQLAlchemyError, TypeError, ValueError, AttributeError) as e:
            self.context.logger.warning(f"Failed to store raw data for debugging: {e}")

    def process_data_type(
        self, source: str, data: Any, processor_func: callable, trigger_id: int, asset_id: int, is_multi: bool
    ) -> dict[str, str]:
        """Process data for a source, handling both single and multi data types."""
        errors: dict[str, str] = {}

        if is_multi:
            if not isinstance(data, dict):
                errors[source] = "Expected dict for multi data type"
            for data_type, subdata in data.items():
                if not subdata:
                    continue
                try:
                    serialized_data = processor_func(subdata)
                    self.store_processed_data(
                        source=source,
                        data_type=data_type,
                        data=serialized_data,
                        trigger_id=trigger_id,
                        asset_id=asset_id,
                    )
                except (TypeError, ValueError) as e:
                    errors[f"{source}_{data_type}"] = f"Data serialization error: {e}"
                except sqlalchemy.exc.SQLAlchemyError as e:
                    errors[f"{source}_{data_type}"] = f"Database error: {e}"
                except AttributeError as e:
                    errors[f"{source}_{data_type}"] = f"Invalid data structure: {e}"
        else:
            if not data:
                return errors
            try:
                serialized_data = processor_func(data)
                self.store_processed_data(
                    source=source,
                    data_type="default",
                    data=serialized_data,
                    trigger_id=trigger_id,
                    asset_id=asset_id,
                )
            except (TypeError, ValueError) as e:
                errors[source] = f"Data serialization error: {e}"
            except sqlalchemy.exc.SQLAlchemyError as e:
                errors[source] = f"Database error: {e}"
            except AttributeError as e:
                errors[source] = f"Invalid data structure: {e}"

        return errors

    def store_processed_data(self, source: str, data_type: str, data: Any, trigger_id: int, asset_id: int) -> None:
        """Store processed data in the database."""
        try:
            serialized_data = {"source": source, "data": data, "error": None}
            self.context.logger.info(
                "Storing processed data for source=%s, data_type=%s, type(data)=%s, data=%s",
                source,
                data_type,
                type(data),
                repr(data)[:500],
            )
            self.context.db_model.store_raw_data(
                source=source,
                data_type=data_type,
                data=serialized_data,
                trigger_id=trigger_id,
                timestamp=datetime.now(tz=UTC),
                asset_id=asset_id,
            )
        except sqlalchemy.exc.SQLAlchemyError as e:
            self.context.logger.warning(f"Database error storing {source} data: {e}")
            raise
        except (TypeError, ValueError) as e:
            self.context.logger.warning(f"Data serialization error for {source}: {e}")
            raise
        except AttributeError as e:
            self.context.logger.warning(f"Invalid data structure for {source}: {e}")
            raise

    def _handle_processing_error(self, errors: dict, key: str, exc: Exception, error_type: str) -> None:
        """Generic error handler for processing."""
        errors[key] = f"{error_type}: {exc!s}"
        self.context.logger.warning(f"{error_type} for {key}: {exc}")

    def build_structured_payload(self, context) -> StructuredPayload:
        """Build and validate the StructuredPayload using Pydantic models."""
        trendmoon_raw = context.raw_data.get("trendmoon", {})
        tree_raw = context.raw_data.get("treeofalpha", [])
        look_raw = context.raw_data.get("lookonchain")
        research_raw = context.raw_data.get("researchagent")

        social_data, coin_details = self._extract_trendmoon_social_and_coin_details(trendmoon_raw)
        trend_market_data = self._extract_trend_market_data(social_data)
        key_metrics = self._build_key_metrics(trend_market_data)
        social_summary = self._build_social_summary(social_data)
        asset_info = self._build_asset_info(coin_details, social_data, context)
        news_items = self._build_news_items(tree_raw)
        official_updates = self._build_official_updates(look_raw)
        onchain_highlights = self._build_onchain_highlights(research_raw, context)

        return StructuredPayload(
            asset_info=asset_info,
            trigger_info=TriggerInfo(
                type="manual_test",
                timestamp=datetime.now(tz=UTC).isoformat(),
            ),
            key_metrics=key_metrics,
            social_summary=social_summary,
            recent_news=news_items,
            onchain_highlights=onchain_highlights,
            official_updates=official_updates,
        )

    def _extract_trendmoon_social_and_coin_details(self, trendmoon_raw):
        if isinstance(trendmoon_raw, dict):
            social_data = trendmoon_raw.get("social", {})
            coin_details = trendmoon_raw.get("coin_details", {})
        else:
            social_data = {}
            coin_details = {}
        if isinstance(social_data, list):
            social_data = social_data[0] if social_data else {}
        return social_data, coin_details

    def _extract_trend_market_data(self, social_data):
        return social_data.get("trend_market_data", [])

    def _build_key_metrics(self, trend_market_data):
        def get_latest_two(entries, key):
            filtered = [e for e in entries if key in e and isinstance(e[key], int | float) and e[key] is not None]
            filtered.sort(key=lambda x: datetime.fromisoformat(x["date"]), reverse=True)
            return filtered[:2]

        # Price change 24h
        price_points = get_latest_two(trend_market_data, "price")
        if len(price_points) == 2:
            latest_price = float(price_points[0]["price"])
            previous_price = float(price_points[1]["price"])
            price_change_24h = ((latest_price - previous_price) / previous_price) * 100 if previous_price else 0.0
        else:
            price_change_24h = 0.0

        # Volume change 24h
        volume_points = get_latest_two(trend_market_data, "total_volume")
        if len(volume_points) == 2:
            latest_volume = float(volume_points[0]["total_volume"])
            previous_volume = float(volume_points[1]["total_volume"])
            volume_change_24h = ((latest_volume - previous_volume) / previous_volume) * 100 if previous_volume else 0.0
        else:
            volume_change_24h = 0.0

        # Mindshare 1h (latest non-null social_dominance)
        mindshare_1h = 0.0
        mindshare_points = [
            e
            for e in sorted(trend_market_data, key=lambda x: datetime.fromisoformat(x["date"]), reverse=True)
            if "social_dominance" in e
            and isinstance(e["social_dominance"], int | float)
            and e["social_dominance"] is not None
        ]
        if mindshare_points:
            mindshare_1h = float(mindshare_points[0]["social_dominance"])

        return KeyMetrics(
            mindshare_1h=mindshare_1h,
            volume_change_24h=volume_change_24h,
            price_change_24h=price_change_24h,
        )

    def _build_social_summary(self, social_data):
        return SocialSummary(
            sentiment_score=social_data.get("lc_sentiment", 0.0),
            top_keywords=social_data.get("symbols", []),
            recent_mention_count=social_data.get("lc_social_volume_24h", 0),
        )

    def _build_asset_info(self, coin_details, social_data, context):
        asset_source = coin_details or social_data
        return AssetInfo(
            name=asset_source.get("name", context.trigger_context.get("asset_name", "")),
            symbol=asset_source.get("symbol", context.trigger_context.get("asset_symbol", "")),
            category=asset_source.get("categories", [None])[0] if asset_source.get("categories") else None,
            coin_id=asset_source.get("id") or asset_source.get("coin_id"),
            market_cap=asset_source.get("market_cap"),
            market_cap_rank=asset_source.get("market_cap_rank"),
            contract_address=asset_source.get("contract_address"),
        )

    def _build_news_items(self, tree_raw):
        return [
            NewsItem(
                headline=r.get("title", ""),
                snippet=r.get("content", r.get("title", "")),
                url=r.get("url", ""),
                timestamp=datetime.fromtimestamp(r["time"] / 1000, UTC).isoformat() if r.get("time") else None,
                source=r.get("source", ""),
            )
            for r in tree_raw
        ]

    def _build_official_updates(self, look_raw):
        return [
            OfficialUpdate(
                timestamp=r.timestamp if hasattr(r, "timestamp") else r.get("timestamp"),
                source=r.source if hasattr(r, "source") else r.get("source"),
                title=r.title if hasattr(r, "title") else r.get("title"),
                snippet=r.summary if hasattr(r, "summary") else r.get("summary"),
            )
            for r in look_raw or []
        ]

    def _build_onchain_highlights(self, research_raw, context):
        highlights = []
        # Flatten tweets from researchagent's data structure
        tweets = []
        for entry in research_raw or []:
            # Each entry is expected to have a 'data' dict with a 'tweets' list
            if isinstance(entry, dict) and "data" in entry and "tweets" in entry["data"]:
                tweets.extend(entry["data"]["tweets"])
        for tweet in tweets:
            ts = tweet.get("timestamp")
            parsed_ts = self._parse_timestamp(ts, context)
            if not isinstance(parsed_ts, datetime):
                context.logger.warning(f"Skipping OnchainHighlight due to missing or invalid timestamp: {ts!r}")
                continue
            highlights.append(
                OnchainHighlight(
                    timestamp=parsed_ts,
                    source=tweet.get("account", "researchagent"),
                    headline=tweet.get("html", ""),
                    snippet="",
                    details=tweet.get("html", ""),
                    event="tweet",
                )
            )
        return highlights

    def _parse_timestamp(self, ts, context):
        if isinstance(ts, str) and ts.isdigit():
            try:
                return datetime.fromtimestamp(int(ts), UTC)
            except (ValueError, TypeError) as e:
                context.logger.warning(f"Failed to parse unix timestamp '{ts}': {e}")
        elif isinstance(ts, int | float):
            try:
                return datetime.fromtimestamp(ts, UTC)
            except (ValueError, TypeError, OSError) as e:
                context.logger.warning(f"Failed to parse numeric timestamp '{ts}': {e}")
        elif isinstance(ts, str):
            try:
                return datetime.fromisoformat(ts)
            except (ValueError, TypeError) as e:
                context.logger.warning(f"Failed to parse ISO timestamp '{ts}': {e}")
        return None

    def act(self) -> None:
        """Process raw data → validate/structure → store."""
        self.context.logger.info(f"Entering state: {self._state}")
        trigger_id = self.context.trigger_context.get("trigger_id")
        asset_id = self.context.trigger_context.get("asset_id")

        # Log missing or errored sources
        expected_sources = set(DATA_SOURCES.keys())
        received_sources = set(self.context.raw_data.keys())
        expected_sources - received_sources

        for source in expected_sources:
            data = self.context.raw_data.get(source)
            if not data:
                error_info = getattr(self.context, "raw_errors", {}).get(source)
                self.context.logger.warning(
                    f"Missing or empty data for source '{source}'."
                    f"{' Error: ' + str(error_info) if error_info else ''}"
                )
                if error_info and error_info.get("http_response"):
                    self.context.logger.warning(f"HTTP response for '{source}': {error_info['http_response']}")

        try:
            self._store_all_raw_data(trigger_id, asset_id)
            payload = self._try_build_structured_payload()
            if payload is None:
                msg = "Failed to build structured payload"
                raise ValidationError(msg)
            if not self._store_structured_payload(payload, trigger_id, asset_id):
                msg = "Failed to store structured payload"
                raise RuntimeError(msg)
            self._event = DyorabciappEvents.DONE

        except ValidationError as e:
            self.context.logger.exception(f"Validation error: {e}")
            self.context.error_context = {
                "error_type": "validation_error",
                "error_message": str(e),
                "error_source": "payload_validation",
                "trigger_id": trigger_id,
                "asset_id": asset_id,
                "recoverable": False,
            }
            self._event = DyorabciappEvents.ERROR

        except (sqlalchemy.exc.SQLAlchemyError, RuntimeError) as e:
            self.context.logger.exception(f"Storage error: {e}")
            self.context.error_context = {
                "error_type": "storage_error",
                "error_message": str(e),
                "error_source": "database_operation",
                "trigger_id": trigger_id,
                "asset_id": asset_id,
                "recoverable": True,
            }
            self._event = DyorabciappEvents.ERROR

        except Exception as e:
            self.context.logger.exception(f"Unexpected error: {e}")
            self.context.error_context = {
                "error_type": "unexpected_error",
                "error_message": str(e),
                "error_source": "process_data",
                "trigger_id": trigger_id,
                "asset_id": asset_id,
                "recoverable": False,
            }
            self._event = DyorabciappEvents.ERROR

        finally:
            self._is_done = True

    def _try_build_structured_payload(self):
        """Build the payload, catch and log all validation errors in detail."""
        try:
            return self.build_structured_payload(self.context)
        except ValidationError as e:
            validation_errors = []
            for err in e.errors():
                loc = ".".join(str(path_item) for path_item in err.get("loc", []))
                msg = err.get("msg", "Unknown error")
                typ = err.get("type", "Unknown type")
                val = err.get("input", "Unknown value")
                validation_errors.append({"location": loc, "message": msg, "error_type": typ, "invalid_value": val})
                self.context.logger.exception(f"Validation error at {loc}: {msg} (type: {typ}, value: {val})")

            self.context.error_context = {
                "error_type": "validation_error",
                "error_source": "payload_schema",
                "validation_errors": validation_errors,
                "raw_data_sample": self._get_safe_data_sample(),
            }
            msg = "Payload validation failed"
            raise ValidationError(msg) from e

    def _get_safe_data_sample(self):
        """Create a safe sample of raw data for error context."""
        try:
            # Create a sample that won't blow up logs but provides context
            sample = {}
            for source, data in self.context.raw_data.items():
                if isinstance(data, dict):
                    sample[source] = dict.fromkeys(data.keys(), "present")
                elif isinstance(data, list):
                    sample[source] = f"list with {len(data)} items"
                else:
                    sample[source] = str(type(data))
            return sample
        except (TypeError, AttributeError, KeyError):
            return "Could not sample raw data"

    def _store_structured_payload(self, payload, trigger_id, asset_id):
        """Store the structured payload in the database."""
        try:
            payload_dict = payload.model_dump(mode="json")
            self.context.db_model.store_raw_data(
                source="structured",
                data_type="default",
                data=payload_dict,
                trigger_id=trigger_id,
                timestamp=datetime.now(tz=UTC),
                asset_id=asset_id,
            )
            return True
        except sqlalchemy.exc.SQLAlchemyError as e:
            self.context.logger.exception(f"Database error: {e}")
            self.context.error_context = {
                "error_type": "database_error",
                "error_source": "store_payload",
                "error_message": str(e),
                "query_info": "store_raw_data",
            }
            msg = f"Database error while storing structured payload: {e}"
            raise RuntimeError(msg) from e
        except TypeError as e:
            self.context.logger.exception(f"Serialization error: {e}")
            self.context.error_context = {
                "error_type": "serialization_error",
                "error_source": "payload_serialization",
                "error_message": str(e),
            }
            msg = f"Failed to serialize payload: {e}"
            raise RuntimeError(msg) from e


class SetupDYORRound(BaseState):
    """This class implements the behaviour of the state SetupDYORRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.SETUPDYORROUND

    def act(self) -> None:
        """Act:
        1. Retrieve DB connection parameters from context/config.
        2. Construct the database URL.
        3. Create the SQLAlchemy engine (connection pool).
        4. Test the connection.
        5. Set DONE event on success, ERROR on failure.
        """
        self.context.logger.info(f"In state: {self._state}")

        try:
            # Setup database connection
            self.context.db_model.setup()

            is_valid, error_msg = self.context.strategy.validate_database_schema()
            if not is_valid:
                raise ValueError(error_msg)

            # Initialize API clients
            self.context.api_clients = {}
            errors = {}

            try:
                self.context.api_clients["lookonchain"] = self.context.lookonchain_client
            except ValueError as e:
                errors["lookonchain_init"] = str(e)

            try:
                self.context.api_clients["treeofalpha"] = self.context.treeofalpha_client
            except ValueError as e:
                errors["treeofalpha_init"] = str(e)

            try:
                self.context.api_clients["researchagent"] = self.context.researchagent_client
            except ValueError as e:
                errors["researchagent_init"] = str(e)

            try:
                self.context.api_clients["trendmoon"] = self.context.trendmoon_client
            except ValueError as e:
                errors["trendmoon_init"] = str(e)

            if errors:
                self.context.logger.warning(f"Failed to initialize API clients: {errors}")
                self._event = DyorabciappEvents.ERROR
            else:
                self.context.logger.info("Successfully initialized API clients")
                self._event = DyorabciappEvents.DONE

        except ValueError as e:
            self.context.logger.exception(f"Configuration error during DB setup: {e}")
            self._event = DyorabciappEvents.ERROR

        except (sqlalchemy.exc.SQLAlchemyError, requests.exceptions.RequestException) as e:
            self.context.logger.exception(f"Unexpected error during DB setup: {e}")
            self._event = DyorabciappEvents.ERROR

        self._is_done = True  # Mark state as done


class DeliverReportRound(BaseState):
    """This class implements the behaviour of the state DeliverReportRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.DELIVERREPORTROUND


class TriggerRound(BaseState):
    """This class implements the behaviour of the state TriggerRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.TRIGGERROUND
        self.test_asset_symbol = kwargs.get("test_asset_symbol")
        self.test_asset_name = kwargs.get("test_asset_name")
        if not self.test_asset_symbol or not self.test_asset_name:
            msg = "test_asset_symbol and test_asset_name must be provided"
            raise ValueError(msg)

    def act(self) -> None:
        """Act on triggers."""
        self.context.logger.info(f"Entering state: {self._state}")

        try:
            # First ensure the asset exists in the database
            with self.context.db_model.engine.connect() as conn:
                # Try to get existing asset first
                result = conn.execute(
                    text("SELECT asset_id FROM assets WHERE symbol = :symbol"), {"symbol": self.test_asset_symbol}
                )
                asset = result.fetchone()

                if asset is None:
                    # Asset doesn't exist, create it
                    result = conn.execute(
                        text("""
                            INSERT INTO assets (symbol, name)
                            VALUES (:symbol, :name)
                            RETURNING asset_id
                        """),
                        {"symbol": self.test_asset_symbol, "name": self.test_asset_name},
                    )
                    asset_id = result.scalar_one()
                    conn.commit()
                    self.context.logger.info(f"Created new asset with ID {asset_id}")
                else:
                    asset_id = asset[0]
                    self.context.logger.info(f"Using existing asset with ID {asset_id}")

            # Now create the trigger with the valid asset_id
            trigger_id = self.context.db_model.create_trigger(
                asset_id=asset_id,  # Use the asset_id we just got/created
                trigger_type="manual_test",
                trigger_details={"source": "test", "description": "Test trigger for data ingestion"},
            )

            self.context.trigger_context = {
                "trigger_id": trigger_id,
                "asset_symbol": self.test_asset_symbol,
                "asset_name": self.test_asset_name,
                "asset_id": asset_id,
            }

            self.context.logger.info(
                f"Created test trigger {trigger_id} for asset {self.test_asset_symbol} (ID: {asset_id})"
            )

            # Update metrics
            self.context.strategy.increment_active_triggers()

            self._event = DyorabciappEvents.DONE

        except Exception as e:
            self.context.logger.exception(f"Error creating trigger: {e}")
            self._event = DyorabciappEvents.ERROR

        self._is_done = True  # Mark state as done


class IngestDataRound(BaseState):
    """This class implements the behaviour of the state IngestDataRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.INGESTDATAROUND
        self._max_workers = kwargs.pop("max_workers", None)
        if self._max_workers is None:
            msg = "max_workers must be provided"
            raise ValueError(msg)

    def _validate_trigger_context(self) -> tuple[str, int]:
        asset_symbol = self.context.trigger_context.get("asset_symbol")
        trigger_id = self.context.trigger_context.get("trigger_id")
        if not asset_symbol or not trigger_id:
            msg = "Missing asset symbol or trigger ID in trigger context"
            raise ValueError(msg)
        return asset_symbol, trigger_id

    def _initialize_raw_data(self) -> None:
        self.context.raw_data = {
            source: {} if config.get("data_type_handler") == "multi" else None
            for source, config in DATA_SOURCES.items()
        }

    def _create_fetch_futures(self, executor, asset_symbol, asset_name):
        futures = {}
        for source, config in DATA_SOURCES.items():
            client = self.context.api_clients.get(source)
            if not client:
                continue
            if config.get("data_type_handler") == "multi":
                for endpoint, fetcher in config["fetchers"].items():
                    futures[f"{source}_{endpoint}"] = executor.submit(
                        fetcher, client, asset_symbol, asset_name=asset_name
                    )
            else:
                fetcher = config["fetcher"]
                # researchagent: fetch for both symbol and name if different
                if source == "researchagent" and asset_name and asset_name.lower() != asset_symbol.lower():
                    futures[f"{source}_symbol"] = executor.submit(fetcher, client, asset_symbol, asset_name=None)
                    futures[f"{source}_name"] = executor.submit(fetcher, client, asset_symbol, asset_name=asset_name)
                else:
                    futures[source] = executor.submit(fetcher, client, asset_symbol, asset_name=asset_name)
        return futures

    def _process_future_result(self, name, result, error=None):
        if error:
            if not hasattr(self.context, "raw_errors"):
                self.context.raw_errors = {}
            self.context.raw_errors[name] = error
        if name.startswith("trendmoon"):
            source, data_type = name.split("_", 1)
            self.context.raw_data[source][data_type] = result
        elif name.startswith("researchagent_"):
            if self.context.raw_data["researchagent"] is None:
                self.context.raw_data["researchagent"] = []
            if isinstance(result, list):
                self.context.raw_data["researchagent"].extend(result)
            elif result is not None:
                self.context.raw_data["researchagent"].append(result)
        else:
            self.context.raw_data[name] = result

    def act(self) -> None:
        """Ingest data from all sources."""
        self.context.logger.info(f"Entering state: {self._state}")
        try:
            self._initialize_raw_data()
            asset_symbol, _ = self._validate_trigger_context()
            asset_name = self.context.trigger_context.get("asset_name", None)
            self.context.logger.info(f"Fetching data for symbol: {asset_symbol}")

            with ThreadPoolExecutor(max_workers=self._max_workers) as executor:
                futures = self._create_fetch_futures(executor, asset_symbol, asset_name)
                for name, future in futures.items():
                    try:
                        result = future.result()
                        self._process_future_result(name, result)
                    except (TrendmoonAPIError, TreeOfAlphaAPIError, LookOnChainAPIError, ResearchAgentAPIError) as e:
                        # Try to get HTTP response if available
                        http_response = getattr(e, "response", None)
                        error_dump = {
                            "error": str(e),
                            "http_response": getattr(http_response, "text", None) if http_response else None,
                            "status_code": getattr(http_response, "status_code", None) if http_response else None,
                        }
                        self._process_future_result(name, None, error=error_dump)
                        self.context.logger.warning(f"Error fetching {name}: {e} | HTTP: {error_dump}")

            self._event = DyorabciappEvents.DONE
        except Exception as e:
            self.context.logger.exception(f"Error during data ingestion: {e}")
            self._event = DyorabciappEvents.ERROR
        self._is_done = True


class GenerateReportRound(BaseState):
    """This class implements the behaviour of the state GenerateReportRound."""

    REQUIRED_SECTIONS = [
        "Overview",
        "Key Recent Changes",
        "Recent News/Events",
        "Analysis",
        "Conclusion",
    ]

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.GENERATEREPORTROUND

    def _fetch_structured_payload(self):
        """Fetch the structured payload from the database."""
        trigger_id = self.context.trigger_context.get("trigger_id")
        asset_id = self.context.trigger_context.get("asset_id")

        payload = self.context.db_model.get_structured_payload(trigger_id, asset_id)
        if not payload:
            msg = "No structured payload found for this trigger/asset."
            raise ValueError(msg)
        return payload

    def _validate_markdown(self, text: str) -> bool:
        """Check if the text is valid Markdown (basic check: can be parsed)."""
        try:
            html = markdown.markdown(text)
            return bool(html.strip())
        except Exception:
            return False

    def _check_required_sections(self, text: str) -> list[str]:
        """Return a list of missing required section headers."""
        missing = []
        for section in self.REQUIRED_SECTIONS:
            # Look for a Markdown header (## Section)
            if not re.search(rf"^###\s*{re.escape(section)}", text, re.MULTILINE | re.IGNORECASE):
                missing.append(section)
        return missing

    def act(self) -> None:
        """Generate a report for the given trigger/asset."""
        self.context.logger.info(f"Entering state: {self._state}")
        try:
            # 1. Fetch structured payload
            payload = self._fetch_structured_payload()

            # 2. Build prompt
            prompt_context = payload
            prompt = build_report_prompt(prompt_context)

            self.context.logger.info(f"Prompt: {prompt}")

            # 3. Call LLM and unpack metadata
            model_config = {
                "temperature": 0.7,
                "max_tokens": 1024,
                "top_p": 1.0,
                "frequency_penalty": 0.0,
                "presence_penalty": 0.0,
            }
            llm_result = self.context.llm_service.generate_summary(prompt, model_config)
            llm_output = llm_result["content"]
            llm_model_used = llm_result["llm_model_used"]
            generation_time_ms = llm_result["generation_time_ms"]
            token_usage_dict = llm_result["token_usage"]

            self.context.logger.info(f"LLM output: {llm_output}")

            # 4. Validate LLM output
            if not self._validate_markdown(llm_output):
                msg = "LLM output is not valid Markdown."
                raise ValueError(msg)

            missing_sections = self._check_required_sections(llm_output)
            if missing_sections:
                msg = f"LLM output missing required sections: {', '.join(missing_sections)}"
                raise ValueError(msg)

            # 5. Store the report
            self.context.db_model.store_report(
                trigger_id=self.context.trigger_context.get("trigger_id"),
                asset_id=self.context.trigger_context.get("asset_id"),
                report_content_markdown=llm_output,
                report_data_json=payload,
                llm_model_used=llm_model_used,
                generation_time_ms=generation_time_ms,
                token_usage=token_usage_dict,
            )

            self._event = DyorabciappEvents.DONE

        except Exception as e:
            self.context.logger.exception(f"Error in GenerateReportRound: {e}")
            self.context.error_context = {
                "error_type": "report_generation_error",
                "error_message": str(e),
                "trigger_id": self.context.trigger_context.get("trigger_id"),
                "asset_id": self.context.trigger_context.get("asset_id"),
            }
            self._event = DyorabciappEvents.ERROR

        self._is_done = True


class HandleErrorRound(BaseState):
    """This class implements the behaviour of the state HandleErrorRound."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self._state = DyorabciappStates.HANDLEERRORROUND


class DyorabciappFsmBehaviour(FSMBehaviour):
    """This class implements a simple Finite State Machine behaviour."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        self.register_state(DyorabciappStates.SETUPDYORROUND.value, SetupDYORRound(**kwargs), True)

        self.register_state(DyorabciappStates.WATCHINGROUND.value, WatchingRound(**kwargs))
        self.register_state(DyorabciappStates.PROCESSDATAROUND.value, ProcessDataRound(**kwargs))
        self.register_state(DyorabciappStates.DELIVERREPORTROUND.value, DeliverReportRound(**kwargs))
        self.register_state(DyorabciappStates.TRIGGERROUND.value, TriggerRound(**kwargs))
        self.register_state(DyorabciappStates.INGESTDATAROUND.value, IngestDataRound(**kwargs))
        self.register_state(DyorabciappStates.GENERATEREPORTROUND.value, GenerateReportRound(**kwargs))
        self.register_state(DyorabciappStates.HANDLEERRORROUND.value, HandleErrorRound(**kwargs))

        self.register_transition(
            source=DyorabciappStates.DELIVERREPORTROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.WATCHINGROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.DELIVERREPORTROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.DELIVERREPORTROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.DELIVERREPORTROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.GENERATEREPORTROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.DELIVERREPORTROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.GENERATEREPORTROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.GENERATEREPORTROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.GENERATEREPORTROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.HANDLEERRORROUND.value,
            event=DyorabciappEvents.RETRY,
            destination=DyorabciappStates.WATCHINGROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.INGESTDATAROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.PROCESSDATAROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.INGESTDATAROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.INGESTDATAROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.INGESTDATAROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.PROCESSDATAROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.GENERATEREPORTROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.PROCESSDATAROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.PROCESSDATAROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.PROCESSDATAROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.SETUPDYORROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.WATCHINGROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.SETUPDYORROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.SETUPDYORROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.SETUPDYORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.TRIGGERROUND.value,
            event=DyorabciappEvents.DONE,
            destination=DyorabciappStates.INGESTDATAROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.TRIGGERROUND.value,
            event=DyorabciappEvents.ERROR,
            destination=DyorabciappStates.HANDLEERRORROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.TRIGGERROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.TRIGGERROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.WATCHINGROUND.value,
            event=DyorabciappEvents.NO_TRIGGER,
            destination=DyorabciappStates.WATCHINGROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.WATCHINGROUND.value,
            event=DyorabciappEvents.TIMEOUT,
            destination=DyorabciappStates.WATCHINGROUND.value,
        )
        self.register_transition(
            source=DyorabciappStates.WATCHINGROUND.value,
            event=DyorabciappEvents.TRIGGER,
            destination=DyorabciappStates.TRIGGERROUND.value,
        )

    def setup(self) -> None:
        """Implement the setup."""
        self.context.logger.info("Setting up Dyorabciapp FSM behaviour.")

    def teardown(self) -> None:
        """Implement the teardown."""
        self.context.logger.info("Tearing down Dyorabciapp FSM behaviour.")

    def act(self) -> None:
        """Implement the act."""
        super().act()
        if self.current is None:
            self.context.logger.info("No state to act on.")
            self.terminate()

    def terminate(self) -> None:
        """Implement the termination."""
        os._exit(0)
